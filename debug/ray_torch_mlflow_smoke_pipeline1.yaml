# PIPELINE DEFINITION
# Name: ray-torch-mlflow-smoke-pipeline
# Inputs:
#    experiment_name: str [Default: 'kubeflow-ray-smoke']
#    mlflow_tracking_uri: str [Default: 'http://mlflow-tracking.mlflow.svc.cluster.local:5000']
#    ray_address: str [Default: 'ray://kuberay-raycluster-head-svc.development.svc.cluster.local:10001']
#    ray_namespace: str [Default: 'default']
components:
  comp-ray-torch-mlflow-smoke-test:
    executorLabel: exec-ray-torch-mlflow-smoke-test
    inputDefinitions:
      parameters:
        experiment_name:
          defaultValue: kubeflow-ray-smoke
          isOptional: true
          parameterType: STRING
        mlflow_tracking_uri:
          parameterType: STRING
        ray_address:
          parameterType: STRING
        ray_namespace:
          defaultValue: default
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-ray-torch-mlflow-smoke-test:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - ray_torch_mlflow_smoke_test
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'mlflow==2.14.1'\
          \  &&  python3 -m pip install --quiet --no-warn-script-location 'kfp==2.14.2'\
          \ '--no-deps' 'typing-extensions>=3.7.4,<5; python_version<\"3.9\"' && \"\
          $0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef ray_torch_mlflow_smoke_test(\n    ray_address: str,\n    mlflow_tracking_uri:\
          \ str,\n    experiment_name: str = \"kubeflow-ray-smoke\",\n    ray_namespace:\
          \ str = \"default\",\n) -> str:\n    import os\n    import sys\n    import\
          \ platform\n    import json\n    import tempfile\n    import subprocess\n\
          \    import socket\n    from urllib.parse import urlparse\n\n    def p(msg):\n\
          \        # simple, consistent print to avoid formatting pitfalls\n     \
          \   print(msg)\n\n    p(\"=== DRIVER ENV INFO (before Ray connect) ===\"\
          )\n    p(\"python: {}\".format(sys.executable))\n    p(\"python_version:\
          \ {}\".format(sys.version))\n    p(\"platform: {}\".format(platform.platform()))\n\
          \    p(\"ray_address(param): {}\".format(ray_address))\n    p(\"mlflow_tracking_uri(param):\
          \ {}\".format(mlflow_tracking_uri))\n    p(\"ray_namespace(param): {}\"\
          .format(ray_namespace))\n    p(\"env.RAY_ADDRESS: {}\".format(os.environ.get('RAY_ADDRESS')))\n\
          \    p(\"env.MLFLOW_TRACKING_URI: {}\".format(os.environ.get('MLFLOW_TRACKING_URI')))\n\
          \n    try:\n        pip_freeze = subprocess.check_output([sys.executable,\
          \ \"-m\", \"pip\", \"freeze\"]).decode()\n    except Exception as e:\n \
          \       pip_freeze = \"<pip freeze failed: {}>\".format(e)\n    p(\"pip\
          \ freeze (driver) \u2014 first 50 lines:\")\n    for line in pip_freeze.splitlines()[:50]:\n\
          \        p(\"  \" + line)\n\n    # Try importing torch ON THE DRIVER (best-effort)\n\
          \    try:\n        import torch as _torch  # noqa: F401\n        driver_has_torch\
          \ = True\n        driver_torch_detail = _torch.__version__\n    except Exception\
          \ as e:\n        driver_has_torch = False\n        driver_torch_detail =\
          \ \"import torch failed on driver: {}\".format(e)\n    p(\"driver_has_torch={};\
          \ detail={}\".format(driver_has_torch, driver_torch_detail))\n\n    import\
          \ ray\n\n    # Sanitize params\n    ray_address = (ray_address or \"\").strip()\n\
          \    mlflow_tracking_uri = (mlflow_tracking_uri or \"\").strip()\n    ray_namespace\
          \ = (ray_namespace or \"\").strip()\n\n    # Preflight TCP check for Ray\
          \ Client port\n    u = urlparse(ray_address)\n    host = u.hostname or ray_address\n\
          \    port = u.port or 10001\n    p(\"=== Preflight: TCP connect to {}:{}\
          \ ===\".format(host, port))\n    try:\n        sock = socket.create_connection((host,\
          \ port), timeout=5)\n        sock.close()\n        p(\"Preflight OK: TCP\
          \ reachable.\")\n    except Exception as e:\n        p(\"[FATAL] Cannot\
          \ reach Ray client at {}:{}\".format(host, port))\n        p(\"Error: {}\"\
          .format(e))\n        p(\"Hints: Service/Endpoints 10001, Istio mTLS/sidecar,\
          \ AuthorizationPolicy/NetworkPolicy.\")\n        sys.exit(1)\n\n    # Connect\
          \ to Ray\n    p(\"=== Connecting to Ray Cluster ===\")\n    try:\n     \
          \   ray.init(address=ray_address, namespace=ray_namespace, ignore_reinit_error=True,\
          \ log_to_driver=True)\n        p(\"Connected to Ray.\")\n        p(\"Ray\
          \ version: {}\".format(ray.__version__))\n        p(\"Ray cluster resources:\
          \ {}\".format(ray.cluster_resources()))\n        p(\"Ray nodes:\")\n   \
          \     for n in ray.nodes():\n            p(\" - {} {} {}\".format(n.get(\"\
          NodeManagerAddress\"), n.get(\"Alive\"), n.get(\"NodeName\")))\n    except\
          \ Exception as e:\n        import traceback as _tb\n        p(\"[FATAL]\
          \ ray.init failed: {}\".format(repr(e)))\n        _tb.print_exc()\n    \
          \    sys.exit(1)\n\n    @ray.remote\n    def torch_probe():\n        info\
          \ = {}\n        try:\n            import torch\n            info[\"torch_import_ok\"\
          ] = True\n            info[\"torch_version\"] = torch.__version__\n    \
          \        cuda = torch.cuda.is_available()\n            info[\"cuda_available\"\
          ] = cuda\n            info[\"cuda_device_count\"] = torch.cuda.device_count()\
          \ if cuda else 0\n            x = torch.randn(8, 4) @ torch.randn(4, 3)\n\
          \            info[\"sample_tensor_mean\"] = float(x.mean().item())\n   \
          \     except Exception as e:\n            info[\"torch_import_ok\"] = False\n\
          \            info[\"error\"] = str(e)\n        import platform as _pf, os\
          \ as _os, sys as _sys\n        info[\"python_version\"] = _sys.version\n\
          \        info[\"hostname\"] = _pf.node()\n        info[\"pid\"] = _os.getpid()\n\
          \        return info\n\n    p(\"=== Probing torch on a Ray worker ===\"\
          )\n    worker_probe = ray.get(torch_probe.remote())\n    p(\"worker_probe:\
          \ {}\".format(json.dumps(worker_probe, indent=2)))\n\n    # Optional tiny\
          \ TorchTrainer\n    trainer_result = None\n    trainer_error = None\n  \
          \  try:\n        from ray.train import ScalingConfig\n        from ray.train.torch\
          \ import TorchTrainer\n\n        def train_loop_per_worker():\n        \
          \    import torch\n            model = torch.nn.Linear(10, 2)\n        \
          \    opt = torch.optim.SGD(model.parameters(), lr=0.01)\n            loss_fn\
          \ = torch.nn.CrossEntropyLoss()\n            x = torch.randn(16, 10)\n \
          \           y = torch.randint(0, 2, (16,))\n            for _ in range(2):\n\
          \                opt.zero_grad(); out = model(x); loss = loss_fn(out, y);\
          \ loss.backward(); opt.step()\n            return {\"final_loss\": float(loss.detach().cpu().item())}\n\
          \n        trainer = TorchTrainer(\n            train_loop_per_worker=train_loop_per_worker,\n\
          \            scaling_config=ScalingConfig(num_workers=1, use_gpu=worker_probe.get(\"\
          cuda_available\", False)),\n        )\n        trainer_result = trainer.fit()\n\
          \        p(\"TorchTrainer result: {}\".format(trainer_result))\n    except\
          \ Exception as e:\n        trainer_error = str(e)\n        p(\"TorchTrainer\
          \ failed: {}\".format(trainer_error))\n\n    # MLflow logging\n    import\
          \ mlflow\n    p(\"=== Logging to MLflow ===\")\n    mlflow.set_tracking_uri(mlflow_tracking_uri)\n\
          \    mlflow.set_experiment(experiment_name)\n    with mlflow.start_run(run_name=\"\
          ray-torch-mlflow-smoke\") as run:\n        mlflow.log_param(\"ray_address\"\
          , ray_address)\n        mlflow.log_param(\"ray_namespace\", ray_namespace)\n\
          \        mlflow.log_param(\"driver_has_torch\", driver_has_torch)\n    \
          \    mlflow.log_param(\"driver_torch_detail\", driver_torch_detail)\n  \
          \      mlflow.log_param(\"worker_hostname\", worker_probe.get(\"hostname\"\
          ))\n        mlflow.log_param(\"worker_python\", worker_probe.get(\"python_version\"\
          ))\n        if \"sample_tensor_mean\" in worker_probe:\n            mlflow.log_metric(\"\
          worker_sample_tensor_mean\", float(worker_probe[\"sample_tensor_mean\"]))\n\
          \        mlflow.log_metric(\"worker_cuda_available\", int(bool(worker_probe.get(\"\
          cuda_available\", False))))\n        mlflow.log_metric(\"worker_cuda_device_count\"\
          , float(worker_probe.get(\"cuda_device_count\", 0)))\n        if trainer_result\
          \ and getattr(trainer_result, \"metrics\", None):\n            try:\n  \
          \              mlflow.log_metric(\"trainer_final_loss\", float(trainer_result.metrics.get(\"\
          final_loss\", -1)))\n            except Exception:\n                pass\n\
          \        debug_payload = {\n            \"driver\": {\n                \"\
          python\": sys.executable,\n                \"python_version\": sys.version,\n\
          \                \"platform\": platform.platform(),\n                \"\
          driver_has_torch\": driver_has_torch,\n                \"driver_torch_detail\"\
          : driver_torch_detail,\n                \"pip_freeze\": pip_freeze,\n  \
          \          },\n            \"ray\": {\n                \"version\": ray.__version__,\n\
          \                \"cluster_resources\": ray.cluster_resources(),\n     \
          \           \"nodes\": ray.nodes(),\n            },\n            \"worker_probe\"\
          : worker_probe,\n            \"trainer_error\": trainer_error,\n       \
          \ }\n        fd, tmp_path = tempfile.mkstemp(prefix=\"ray_torch_mlflow_debug_\"\
          , suffix=\".json\")\n        with os.fdopen(fd, \"w\") as f:\n         \
          \   json.dump(debug_payload, f, indent=2)\n        mlflow.log_artifact(tmp_path,\
          \ artifact_path=\"debug\")\n        p(\"MLflow run id: {}\".format(run.info.run_id))\n\
          \n    summary = {\n        \"driver_has_torch\": driver_has_torch,\n   \
          \     \"driver_torch_detail\": driver_torch_detail,\n        \"worker_torch_ok\"\
          : worker_probe.get(\"torch_import_ok\", False),\n        \"worker_torch_version\"\
          : worker_probe.get(\"torch_version\"),\n        \"worker_cuda_available\"\
          : worker_probe.get(\"cuda_available\", False),\n        \"trainer_error\"\
          : trainer_error,\n    }\n    return json.dumps(summary)\n\n"
        image: rayproject/ray:2.48.0-py312-cpu
pipelineInfo:
  name: ray-torch-mlflow-smoke-pipeline
root:
  dag:
    tasks:
      ray-torch-mlflow-smoke-test:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-ray-torch-mlflow-smoke-test
        inputs:
          parameters:
            experiment_name:
              componentInputParameter: experiment_name
            mlflow_tracking_uri:
              componentInputParameter: mlflow_tracking_uri
            ray_address:
              componentInputParameter: ray_address
            ray_namespace:
              componentInputParameter: ray_namespace
        taskInfo:
          name: ray-torch-mlflow-smoke-test
  inputDefinitions:
    parameters:
      experiment_name:
        defaultValue: kubeflow-ray-smoke
        isOptional: true
        parameterType: STRING
      mlflow_tracking_uri:
        defaultValue: http://mlflow-tracking.mlflow.svc.cluster.local:5000
        isOptional: true
        parameterType: STRING
      ray_address:
        defaultValue: ray://kuberay-raycluster-head-svc.development.svc.cluster.local:10001
        isOptional: true
        parameterType: STRING
      ray_namespace:
        defaultValue: default
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.2
