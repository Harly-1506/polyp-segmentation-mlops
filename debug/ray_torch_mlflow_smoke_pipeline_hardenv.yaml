# PIPELINE DEFINITION
# Name: ray-torch-mlflow-smoke-pipeline
# Inputs:
#    experiment_name: str [Default: 'kubeflow-ray-smoke']
#    mlflow_tracking_uri: str [Default: 'http://mlflow-tracking-service.mlflow.svc.cluster.local:5000']
#    ray_address: str [Default: 'ray://kuberay-raycluster-head-svc.development.svc.cluster.local:10001']
#    ray_namespace: str [Default: 'default']
components:
  comp-ray-torch-mlflow-smoke-test:
    executorLabel: exec-ray-torch-mlflow-smoke-test
    inputDefinitions:
      parameters:
        experiment_name:
          defaultValue: kubeflow-ray-smoke
          isOptional: true
          parameterType: STRING
        mlflow_tracking_uri:
          defaultValue: http://mlflow-tracking-service.mlflow.svc.cluster.local:5000
          isOptional: true
          parameterType: STRING
        ray_address:
          defaultValue: ray://kuberay-raycluster-head-svc.development.svc.cluster.local:10001
          isOptional: true
          parameterType: STRING
        ray_namespace:
          defaultValue: default
          isOptional: true
          parameterType: STRING
    outputDefinitions:
      parameters:
        Output:
          parameterType: STRING
deploymentSpec:
  executors:
    exec-ray-torch-mlflow-smoke-test:
      container:
        args:
        - --executor_input
        - '{{$}}'
        - --function_to_execute
        - ray_torch_mlflow_smoke_test
        command:
        - sh
        - -c
        - "\nif ! [ -x \"$(command -v pip)\" ]; then\n    python3 -m ensurepip ||\
          \ python3 -m ensurepip --user || apt-get install python3-pip\nfi\n\nPIP_DISABLE_PIP_VERSION_CHECK=1\
          \ python3 -m pip install --quiet --no-warn-script-location 'mlflow==3.1.0'\
          \ 'pyarrow>=16.1,<21' 'torch==2.7.1'  &&  python3 -m pip install --quiet\
          \ --no-warn-script-location 'kfp==2.14.2' '--no-deps' 'typing-extensions>=3.7.4,<5;\
          \ python_version<\"3.9\"' && \"$0\" \"$@\"\n"
        - sh
        - -ec
        - 'program_path=$(mktemp -d)


          printf "%s" "$0" > "$program_path/ephemeral_component.py"

          _KFP_RUNTIME=true python3 -m kfp.dsl.executor_main                         --component_module_path                         "$program_path/ephemeral_component.py"                         "$@"

          '
        - "\nimport kfp\nfrom kfp import dsl\nfrom kfp.dsl import *\nfrom typing import\
          \ *\n\ndef ray_torch_mlflow_smoke_test(\n    # B\u1EA1n v\u1EABn c\xF3 th\u1EC3\
          \ override b\u1EB1ng tham s\u1ED1 pipeline n\u1EBFu mu\u1ED1n\n    ray_address:\
          \ str = \"ray://kuberay-raycluster-head-svc.development.svc.cluster.local:10001\"\
          ,\n    mlflow_tracking_uri: str = \"http://mlflow-tracking-service.mlflow.svc.cluster.local:5000\"\
          ,\n    experiment_name: str = \"kubeflow-ray-smoke\",\n    ray_namespace:\
          \ str = \"default\",\n) -> str:\n    import os, sys, json, socket, platform,\
          \ tempfile, subprocess, traceback, time\n    from urllib.parse import urlparse\n\
          \n    # ======== HARD-CODE ENV FOR SMOKE TEST ========\n    # N\u1EBFu mu\u1ED1\
          n \u0111\u1ED5i, s\u1EEDa 4 d\xF2ng d\u01B0\u1EDBi:\n    os.environ[\"MLFLOW_TRACKING_URI\"\
          ] = \"http://mlflow-tracking-service.mlflow.svc.cluster.local:5000\"\n \
          \   os.environ[\"MLFLOW_S3_ENDPOINT_URL\"] = \"http://minio-service.mlflow.svc.cluster.local:9000\"\
          \n    os.environ[\"AWS_ACCESS_KEY_ID\"] = \"admin\"\n    os.environ[\"AWS_SECRET_ACCESS_KEY\"\
          ] = \"admin1234\"\n    # Khuy\u1EBFn ngh\u1ECB th\xEAm:\n    os.environ[\"\
          AWS_DEFAULT_REGION\"] = \"us-east-1\"\n    os.environ[\"AWS_EC2_METADATA_DISABLED\"\
          ] = \"true\"\n    os.environ[\"MLFLOW_S3_IGNORE_TLS\"] = os.environ.get(\"\
          MLFLOW_S3_IGNORE_TLS\", \"true\")\n\n    def p(msg: str) -> None:\n    \
          \    print(msg, flush=True)\n\n    def _json_roundtrip(obj):\n        return\
          \ json.loads(json.dumps(obj))\n\n    # ======== DRIVER ENV INFO ========\n\
          \    p(\"=== DRIVER ENV INFO (before Ray connect) ===\")\n    p(f\"python:\
          \ {sys.executable}\")\n    p(f\"python_version: {sys.version}\")\n    p(f\"\
          platform: {platform.platform()}\")\n    p(f\"ray_address(effect): {ray_address}\"\
          )\n    p(f\"ray_namespace(effect): {ray_namespace}\")\n    p(f\"MLFLOW_TRACKING_URI:\
          \ {os.environ.get('MLFLOW_TRACKING_URI')}\")\n    p(f\"MLFLOW_S3_ENDPOINT_URL:\
          \ {os.environ.get('MLFLOW_S3_ENDPOINT_URL')}\")\n    try:\n        pf =\
          \ subprocess.check_output([sys.executable, \"-m\", \"pip\", \"freeze\"],\
          \ text=True)\n    except Exception as e:\n        pf = f\"<pip freeze failed:\
          \ {e}>\"\n    for line in pf.splitlines()[:50]:\n        p(\"  \" + line)\n\
          \n    try:\n        import torch as _t\n        drv_has_torch, drv_torch_detail\
          \ = True, _t.__version__\n    except Exception as e:\n        drv_has_torch,\
          \ drv_torch_detail = False, f\"import torch failed on driver: {e}\"\n  \
          \  p(f\"driver_has_torch={drv_has_torch}; detail={drv_torch_detail}\")\n\
          \n    import ray\n\n    # ======== Preflight TCP \u0111\u1EBFn Ray client\
          \ ========\n    u = urlparse(ray_address)\n    host = u.hostname or ray_address\n\
          \    port = u.port or 10001\n    p(f\"=== Preflight: TCP connect to {host}:{port}\
          \ ===\")\n    try:\n        s = socket.create_connection((host, port), timeout=5)\n\
          \        s.close()\n        p(\"Preflight OK: TCP reachable.\")\n    except\
          \ Exception as e:\n        p(f\"[FATAL] Cannot reach Ray client at {host}:{port}\\\
          nError: {e}\")\n        raise RuntimeError(\"Ray TCP preflight failed\"\
          ) from e\n\n    # ======== Connect Ray & propagate ENV sang workers ========\n\
          \    env_to_propagate = {\n        k: os.environ[k]\n        for k in [\n\
          \            \"MLFLOW_TRACKING_URI\",\n            \"MLFLOW_S3_ENDPOINT_URL\"\
          ,\n            \"MLFLOW_S3_IGNORE_TLS\",\n            \"AWS_ACCESS_KEY_ID\"\
          ,\n            \"AWS_SECRET_ACCESS_KEY\",\n            \"AWS_DEFAULT_REGION\"\
          ,\n            \"AWS_EC2_METADATA_DISABLED\",\n        ]\n        if k in\
          \ os.environ\n    }\n\n    p(\"=== Connecting to Ray Cluster ===\")\n  \
          \  ray.init(\n        address=ray_address,\n        namespace=ray_namespace,\n\
          \        ignore_reinit_error=True,\n        log_to_driver=True,\n      \
          \  runtime_env={\"env_vars\": env_to_propagate},\n    )\n    p(\"Connected\
          \ to Ray.\")\n    p(f\"Ray version: {ray.__version__}\")\n    p(f\"Ray cluster\
          \ resources: {ray.cluster_resources()}\")\n\n    # ========== PROBES ==========\n\
          \    @ray.remote(max_retries=0)\n    def ping():\n        return \"pong\"\
          \n\n    p(\"=== Ray ping ===\")\n    p(f\"ping: {ray.get(ping.remote())}\"\
          )\n\n    @ray.remote(max_retries=0)\n    def py_probe():\n        import\
          \ sys as _s, platform as _pl\n        return _json_roundtrip({\"python\"\
          : _s.version, \"executable\": _s.executable, \"platform\": _pl.platform()})\n\
          \n    p(\"=== Python probe on worker ===\")\n    py_env = ray.get(py_probe.remote())\n\
          \    p(json.dumps(py_env, indent=2))\n\n    @ray.remote(max_retries=0)\n\
          \    def numpy_probe():\n        import numpy as np, numpy.core.multiarray\
          \ as _ma\n        return _json_roundtrip({\"numpy_version\": str(np.__version__),\
          \ \"numpy_file\": str(getattr(np, \"__file__\", \"\"))})\n\n    p(\"===\
          \ NumPy probe on worker ===\")\n    np_env = ray.get(numpy_probe.remote())\n\
          \    p(json.dumps(np_env, indent=2))\n\n    @ray.remote(max_retries=0)\n\
          \    def torch_probe():\n        try:\n            import torch\n      \
          \      cuda = bool(torch.cuda.is_available())\n            return _json_roundtrip({\n\
          \                \"torch_ok\": True,\n                \"torch_version\"\
          : str(torch.__version__),\n                \"cuda_available\": cuda,\n \
          \               \"cuda_device_count\": int(torch.cuda.device_count() if\
          \ cuda else 0),\n                \"mean\": float((torch.randn(8, 4) @ torch.randn(4,\
          \ 3)).mean().item()),\n            })\n        except Exception as e:\n\
          \            return _json_roundtrip({\"torch_ok\": False, \"error\": f\"\
          {type(e).__name__}: {e}\"})\n\n    p(\"=== Torch probe on worker ===\")\n\
          \    t_probe = ray.get(torch_probe.remote())\n    p(json.dumps(t_probe,\
          \ indent=2))\n    if not t_probe.get(\"torch_ok\", False):\n        raise\
          \ RuntimeError(f\"Torch probe failed: {t_probe.get('error')}\")\n\n    #\
          \ ========== Tiny trainer (best effort) ==========\n    trainer_result,\
          \ trainer_error = None, None\n    try:\n        from ray.train import ScalingConfig\n\
          \        from ray.train.torch import TorchTrainer\n\n        def train_loop_per_worker():\n\
          \            import torch\n            m = torch.nn.Linear(10, 2)\n    \
          \        opt = torch.optim.SGD(m.parameters(), lr=0.01)\n            loss_fn\
          \ = torch.nn.CrossEntropyLoss()\n            x = torch.randn(16, 10)\n \
          \           y = torch.randint(0, 2, (16,))\n            for _ in range(2):\n\
          \                opt.zero_grad()\n                out = m(x)\n         \
          \       loss = loss_fn(out, y)\n                loss.backward()\n      \
          \          opt.step()\n            return {\"final_loss\": float(loss.detach().cpu().item())}\n\
          \n        use_gpu = bool(t_probe.get(\"cuda_available\"))\n        trainer\
          \ = TorchTrainer(\n            train_loop_per_worker,\n            scaling_config=ScalingConfig(num_workers=1,\
          \ use_gpu=use_gpu, trainer_resources={\"CPU\": 0}),\n        )\n       \
          \ trainer_result = trainer.fit()\n        p(f\"TorchTrainer result: {trainer_result}\"\
          )\n    except Exception as e:\n        trainer_error = str(e)\n        p(f\"\
          TorchTrainer failed: {trainer_error}\")\n\n    # ========== MLflow logging\
          \ ==========\n    import mlflow\n    p(\"=== Logging to MLflow ===\")\n\
          \    mlflow.set_tracking_uri(os.environ[\"MLFLOW_TRACKING_URI\"])\n    mlflow.set_experiment(experiment_name)\n\
          \n    with mlflow.start_run(run_name=\"ray-torch-mlflow-smoke\") as run:\n\
          \        mlflow.log_param(\"ray_address\", ray_address)\n        mlflow.log_param(\"\
          ray_namespace\", ray_namespace)\n        mlflow.log_param(\"driver_has_torch\"\
          , drv_has_torch)\n        mlflow.log_param(\"driver_torch_detail\", drv_torch_detail)\n\
          \n        for k in [\"python\", \"executable\", \"platform\"]:\n       \
          \     mlflow.log_param(\"worker_\" + k, str(py_env.get(k)))\n        mlflow.log_param(\"\
          worker_numpy_version\", np_env.get(\"numpy_version\"))\n        mlflow.log_param(\"\
          worker_numpy_file\", np_env.get(\"numpy_file\"))\n\n        for k in [\"\
          torch_ok\", \"torch_version\", \"cuda_available\", \"cuda_device_count\"\
          , \"mean\"]:\n            if k in t_probe:\n                mlflow.log_param(\"\
          worker_\" + k, str(t_probe[k]))\n\n        if trainer_result and getattr(trainer_result,\
          \ \"metrics\", None):\n            try:\n                mlflow.log_metric(\"\
          trainer_final_loss\", float(trainer_result.metrics.get(\"final_loss\", -1)))\n\
          \            except Exception:\n                pass\n\n        # Ghi debug\
          \ payload l\xE0m artifact (client-side \u2192 d\xF9ng AWS_* \u1EDF tr\xEA\
          n)\n        payload = {\n            \"driver\": {\"python\": sys.version,\
          \ \"executable\": sys.executable, \"platform\": platform.platform(), \"\
          pip_freeze\": pf},\n            \"ray\": {\"version\": ray.__version__,\
          \ \"cluster_resources\": ray.cluster_resources()},\n            \"py_env\"\
          : py_env,\n            \"numpy_env\": np_env,\n            \"torch_probe\"\
          : t_probe,\n            \"trainer_error\": trainer_error,\n        }\n \
          \       import tempfile\n        fd, tmp = tempfile.mkstemp(prefix=\"ray_torch_mlflow_debug_\"\
          , suffix=\".json\")\n        with os.fdopen(fd, \"w\") as f:\n         \
          \   json.dump(payload, f, indent=2)\n        mlflow.log_artifact(tmp, artifact_path=\"\
          debug\")\n        p(f\"MLflow run id: {run.info.run_id}\")\n\n    return\
          \ json.dumps({\"worker_torch_ok\": bool(t_probe.get(\"torch_ok\", False))})\n\
          \n"
        image: rayproject/ray:2.48.0-py312-cpu
pipelineInfo:
  name: ray-torch-mlflow-smoke-pipeline
root:
  dag:
    tasks:
      ray-torch-mlflow-smoke-test:
        cachingOptions:
          enableCache: true
        componentRef:
          name: comp-ray-torch-mlflow-smoke-test
        inputs:
          parameters:
            experiment_name:
              componentInputParameter: experiment_name
            mlflow_tracking_uri:
              componentInputParameter: mlflow_tracking_uri
            ray_address:
              componentInputParameter: ray_address
            ray_namespace:
              componentInputParameter: ray_namespace
        taskInfo:
          name: ray-torch-mlflow-smoke-test
  inputDefinitions:
    parameters:
      experiment_name:
        defaultValue: kubeflow-ray-smoke
        isOptional: true
        parameterType: STRING
      mlflow_tracking_uri:
        defaultValue: http://mlflow-tracking-service.mlflow.svc.cluster.local:5000
        isOptional: true
        parameterType: STRING
      ray_address:
        defaultValue: ray://kuberay-raycluster-head-svc.development.svc.cluster.local:10001
        isOptional: true
        parameterType: STRING
      ray_namespace:
        defaultValue: default
        isOptional: true
        parameterType: STRING
schemaVersion: 2.1.0
sdkVersion: kfp-2.14.2
